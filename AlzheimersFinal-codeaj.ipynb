{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKeVGxZ5GG6o"
      },
      "source": [
        "# Import needed modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "GsqtgjCLwsXc",
        "outputId": "4d67014b-8abe-4549-d6ad-4b1b73d672be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6f9f6ffa-96d2-4b04-a04a-ad484fe3a43f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6f9f6ffa-96d2-4b04-a04a-ad484fe3a43f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Downloading alzheimers-dataset-4-class-of-images.zip to /content\n",
            " 73% 25.0M/34.1M [00:00<00:00, 104MB/s] \n",
            "100% 34.1M/34.1M [00:00<00:00, 115MB/s]\n"
          ]
        }
      ],
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d tourist55/alzheimers-dataset-4-class-of-images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42tgRLm0wxz0",
        "outputId": "9491517b-df02-4178-d9b0-e50a2318b775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: Alzheimer_s Dataset/train/MildDemented/mildDem207.jpg  \n",
            "  inflating: Alzheimer_s Dataset/train/MildDemented/mildDem208.jpg  \n",
            "  inflating: Alzheimer_s Dataset/train/VeryMildDemented/verymildDem999.jpg  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/alzheimers-dataset-4-class-of-images.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CeMcAy_5GG6s",
        "outputId": "7094d4d8-deff-4698-c69b-ae0b5f6f84e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "modules loaded\n"
          ]
        }
      ],
      "source": [
        "# import system libs\n",
        "import os\n",
        "import time\n",
        "import shutil\n",
        "import pathlib\n",
        "import itertools\n",
        "\n",
        "# import data handling tools\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# import Deep learning Libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam, Adamax\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Ignore Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print ('modules loaded')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA_gwvwnGG6v"
      },
      "source": [
        "# Create needed functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4reLHLHabWD"
      },
      "source": [
        "## Functions to Create Data Frame from Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQdhl_CRGG6v"
      },
      "source": [
        "#### **Function to create data frame**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g2nDmYaAabWE"
      },
      "outputs": [],
      "source": [
        "# Generate data paths with labels\n",
        "def define_paths(data_dir):\n",
        "    filepaths = []\n",
        "    labels = []\n",
        "\n",
        "    folds = os.listdir(data_dir)\n",
        "    for fold in folds:\n",
        "        foldpath = os.path.join(data_dir, fold)\n",
        "        filelist = os.listdir(foldpath)\n",
        "        for file in filelist:\n",
        "            fpath = os.path.join(foldpath, file)\n",
        "            filepaths.append(fpath)\n",
        "            labels.append(fold)\n",
        "\n",
        "    return filepaths, labels\n",
        "\n",
        "\n",
        "# Concatenate data paths with labels into one dataframe ( to later be fitted into the model )\n",
        "def define_df(files, classes):\n",
        "    Fseries = pd.Series(files, name= 'filepaths')\n",
        "    Lseries = pd.Series(classes, name='labels')\n",
        "    return pd.concat([Fseries, Lseries], axis= 1)\n",
        "\n",
        "# Split dataframe to train, valid, and test\n",
        "def split_data(data_dir):\n",
        "    # train dataframe\n",
        "    files, classes = define_paths(data_dir)\n",
        "    df = define_df(files, classes)\n",
        "    strat = df['labels']\n",
        "    train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123, stratify= strat)\n",
        "\n",
        "    # valid and test dataframe\n",
        "    strat = dummy_df['labels']\n",
        "    valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)\n",
        "\n",
        "    return train_df, valid_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZaHdeFxGG6x"
      },
      "source": [
        "#### Function to generate images from dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iLL8hHQcGG6x"
      },
      "outputs": [],
      "source": [
        "def create_gens (train_df, valid_df, test_df, batch_size):\n",
        "    '''\n",
        "    This function takes train, validation, and test dataframe and fit them into image data generator, because model takes data from image data generator.\n",
        "    Image data generator converts images into tensors. '''\n",
        "\n",
        "\n",
        "    # define model parameters\n",
        "    img_size = (224, 224)\n",
        "    channels = 3 # either BGR or Grayscale\n",
        "    color = 'rgb'\n",
        "    img_shape = (img_size[0], img_size[1], channels)\n",
        "\n",
        "    # Recommended : use custom function for test data batch size, else we can use normal batch size.\n",
        "    ts_length = len(test_df)\n",
        "    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "    test_steps = ts_length // test_batch_size\n",
        "\n",
        "    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
        "    def scalar(img):\n",
        "        return img\n",
        "\n",
        "    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)\n",
        "    ts_gen = ImageDataGenerator(preprocessing_function= scalar)\n",
        "\n",
        "    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
        "\n",
        "    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= color, shuffle= True, batch_size= batch_size)\n",
        "\n",
        "    # Note: we will use custom test_batch_size, and make shuffle= false\n",
        "    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',\n",
        "                                        color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
        "\n",
        "    return train_gen, valid_gen, test_gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ifXox4SGG6y"
      },
      "source": [
        "#### **Function to display data sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IAGbj3ZyGG6y"
      },
      "outputs": [],
      "source": [
        "def show_images(gen):\n",
        "    '''\n",
        "    This function take the data generator and show sample of the images\n",
        "    '''\n",
        "\n",
        "    # return classes , images to be displayed\n",
        "    g_dict = gen.class_indices        # defines dictionary {'class': index}\n",
        "    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string\n",
        "    images, labels = next(gen)        # get a batch size samples from the generator\n",
        "\n",
        "    # calculate number of displayed samples\n",
        "    length = len(labels)        # length of batch size\n",
        "    sample = min(length, 25)    # check if sample less than 25 images\n",
        "\n",
        "    plt.figure(figsize= (20, 20))\n",
        "\n",
        "    for i in range(sample):\n",
        "        plt.subplot(5, 5, i + 1)\n",
        "        image = images[i] / 255       # scales data to range (0 - 255)\n",
        "        plt.imshow(image)\n",
        "        index = np.argmax(labels[i])  # get image index\n",
        "        class_name = classes[index]   # get class of image\n",
        "        plt.title(class_name, color= 'blue', fontsize= 12)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K-ryg0DGG6z"
      },
      "source": [
        "#### **Callbacks**\n",
        "<br>\n",
        "Callbacks : Helpful functions to help optimize model training  <br>\n",
        "Examples: stop model training after specfic time, stop training if no improve in accuracy and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5HiN8XDGG60"
      },
      "outputs": [],
      "source": [
        "class MyCallback(keras.callbacks.Callback) :\n",
        "    def __init__(self, model, patience, stop_patience, threshold, factor, batches, epochs, ask_epoch):\n",
        "        super(MyCallback, self).__init__()\n",
        "        self.model = model\n",
        "        self.patience = patience # specifies how many epochs without improvement before learning rate is adjusted\n",
        "        self.stop_patience = stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
        "        self.threshold = threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
        "        self.factor = factor # factor by which to reduce the learning rate\n",
        "        self.batches = batches # number of training batch to run per epoch\n",
        "        self.epochs = epochs\n",
        "        self.ask_epoch = ask_epoch\n",
        "        self.ask_epoch_initial = ask_epoch # save this value to restore if restarting training\n",
        "\n",
        "        # callback variables\n",
        "        self.count = 0 # how many times lr has been reduced without improvement\n",
        "        self.stop_count = 0\n",
        "        self.best_epoch = 1   # epoch with the lowest loss\n",
        "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n",
        "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
        "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
        "        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n",
        "        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n",
        "\n",
        "    # Define a function that will run when train begins\n",
        "    def on_train_begin(self, logs= None):\n",
        "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
        "        print(msg)\n",
        "        ans = input('')\n",
        "        if ans in ['Y', 'y']:\n",
        "            self.ask_permission = 1\n",
        "        elif ans in ['N', 'n']:\n",
        "            self.ask_permission = 0\n",
        "\n",
        "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
        "        print(msg)\n",
        "        self.start_time = time.time()\n",
        "\n",
        "\n",
        "    def on_train_end(self, logs= None):\n",
        "        stop_time = time.time()\n",
        "        tr_duration = stop_time - self.start_time\n",
        "        hours = tr_duration // 3600\n",
        "        minutes = (tr_duration - (hours * 3600)) // 60\n",
        "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
        "\n",
        "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
        "        print(msg)\n",
        "\n",
        "        # set the weights of the model to the best weights\n",
        "        self.model.set_weights(self.best_weights)\n",
        "\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs= None):\n",
        "        # get batch accuracy and loss\n",
        "        acc = logs.get('accuracy') * 100\n",
        "        loss = logs.get('loss')\n",
        "\n",
        "        # prints over on the same line to show running batch count\n",
        "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
        "        print(msg, '\\r', end= '')\n",
        "\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs= None):\n",
        "        self.ep_start = time.time()\n",
        "\n",
        "\n",
        "    # Define method runs on the end of each epoch\n",
        "    def on_epoch_end(self, epoch, logs= None):\n",
        "        ep_end = time.time()\n",
        "        duration = ep_end - self.ep_start\n",
        "\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
        "        current_lr = lr\n",
        "        acc = logs.get('accuracy')  # get training accuracy\n",
        "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
        "        loss = logs.get('loss')  # get training loss for this epoch\n",
        "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
        "\n",
        "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
        "            monitor = 'accuracy'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "            else:\n",
        "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
        "\n",
        "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
        "                self.highest_tracc = acc # set new highest training accuracy\n",
        "                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n",
        "                self.count = 0 # set count to 0 since training accuracy improved\n",
        "                self.stop_count = 0 # set stop counter to 0\n",
        "                if v_loss < self.lowest_vloss:\n",
        "                    self.lowest_vloss = v_loss\n",
        "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
        "\n",
        "            else:\n",
        "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
        "                # if so adjust learning rate\n",
        "                if self.count >= self.patience - 1: # lr should be adjusted\n",
        "                    lr = lr * self.factor # adjust the learning by factor\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "                    self.count = 0 # reset the count to 0\n",
        "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
        "                    self.count = 0 # reset counter\n",
        "                    if v_loss < self.lowest_vloss:\n",
        "                        self.lowest_vloss = v_loss\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment patience counter\n",
        "\n",
        "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
        "            monitor = 'val_loss'\n",
        "            if epoch == 0:\n",
        "                pimprov = 0.0\n",
        "\n",
        "            else:\n",
        "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
        "\n",
        "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
        "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
        "                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n",
        "                self.count = 0 # reset count since validation loss improved\n",
        "                self.stop_count = 0\n",
        "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
        "\n",
        "            else: # validation loss did not improve\n",
        "                if self.count >= self.patience - 1: # need to adjust lr\n",
        "                    lr = lr * self.factor # adjust the learning rate\n",
        "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
        "                    self.count = 0 # reset counter\n",
        "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
        "\n",
        "                else:\n",
        "                    self.count = self.count + 1 # increment the patience counter\n",
        "\n",
        "                if acc > self.highest_tracc:\n",
        "                    self.highest_tracc = acc\n",
        "\n",
        "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
        "        print(msg)\n",
        "\n",
        "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
        "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
        "            print(msg)\n",
        "            self.model.stop_training = True # stop training\n",
        "\n",
        "        else:\n",
        "            if self.ask_epoch != None and self.ask_permission != 0:\n",
        "                if epoch + 1 >= self.ask_epoch:\n",
        "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
        "                    print(msg)\n",
        "\n",
        "                    ans = input('')\n",
        "                    if ans == 'H' or ans == 'h':\n",
        "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
        "                        print(msg)\n",
        "                        self.model.stop_training = True # stop training\n",
        "\n",
        "                    else:\n",
        "                        try:\n",
        "                            ans = int(ans)\n",
        "                            self.ask_epoch += ans\n",
        "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
        "                            print(msg)\n",
        "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
        "                            print(msg)\n",
        "\n",
        "                        except Exception:\n",
        "                            print('Invalid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zwhoj3zGG61"
      },
      "source": [
        "#### **Function to plot history of training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pU3eAW5jGG62"
      },
      "outputs": [],
      "source": [
        "def plot_training(hist):\n",
        "    '''\n",
        "    This function take training model and plot history of accuracy and losses with the best epoch in both of them.\n",
        "    '''\n",
        "\n",
        "    # Define needed variables\n",
        "    tr_acc = hist.history['accuracy']\n",
        "    tr_loss = hist.history['loss']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    index_loss = np.argmin(val_loss)\n",
        "    val_lowest = val_loss[index_loss]\n",
        "    index_acc = np.argmax(val_acc)\n",
        "    acc_highest = val_acc[index_acc]\n",
        "    Epochs = [i+1 for i in range(len(tr_acc))]\n",
        "    loss_label = f'best epoch= {str(index_loss + 1)}'\n",
        "    acc_label = f'best epoch= {str(index_acc + 1)}'\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize= (20, 8))\n",
        "    plt.style.use('fivethirtyeight')\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')\n",
        "    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')\n",
        "    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')\n",
        "    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')\n",
        "    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pK6cgu7LGG63"
      },
      "source": [
        "#### **Function to create Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_4mPYHnzGG64"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):\n",
        "\t'''\n",
        "\tThis function plot confusion matrix method from sklearn package.\n",
        "\t'''\n",
        "\n",
        "\tplt.figure(figsize= (10, 10))\n",
        "\tplt.imshow(cm, interpolation= 'nearest', cmap= cmap)\n",
        "\tplt.title(title)\n",
        "\tplt.colorbar()\n",
        "\n",
        "\ttick_marks = np.arange(len(classes))\n",
        "\tplt.xticks(tick_marks, classes, rotation= 45)\n",
        "\tplt.yticks(tick_marks, classes)\n",
        "\n",
        "\tif normalize:\n",
        "\t\tcm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]\n",
        "\t\tprint('Normalized Confusion Matrix')\n",
        "\n",
        "\telse:\n",
        "\t\tprint('Confusion Matrix, Without Normalization')\n",
        "\n",
        "\tprint(cm)\n",
        "\n",
        "\tthresh = cm.max() / 2.\n",
        "\tfor i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "\t\tplt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')\n",
        "\n",
        "\tplt.tight_layout()\n",
        "\tplt.ylabel('True Label')\n",
        "\tplt.xlabel('Predicted Label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57eDFl3oGG65"
      },
      "source": [
        "# **Model Structure**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GHNMVrhGG65"
      },
      "source": [
        "#### **Start Reading Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T18:43:26.756505Z",
          "iopub.status.busy": "2023-03-03T18:43:26.755915Z",
          "iopub.status.idle": "2023-03-03T18:43:27.782218Z",
          "shell.execute_reply": "2023-03-03T18:43:27.781224Z",
          "shell.execute_reply.started": "2023-03-03T18:43:26.756464Z"
        },
        "id": "FWfxfQEVabWS",
        "outputId": "6567d041-e66d-4eb5-92fe-c36a31e97dba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 4096 validated image filenames belonging to 4 classes.\n",
            "Found 512 validated image filenames belonging to 4 classes.\n",
            "Found 513 validated image filenames belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "data_dir = '/content/Alzheimer_s Dataset/train'\n",
        "\n",
        "try:\n",
        "    # Get splitted data\n",
        "    train_df, valid_df, test_df = split_data(data_dir)\n",
        "\n",
        "    # Get Generators\n",
        "    batch_size = 40\n",
        "    train_gen, valid_gen, test_gen = create_gens(train_df, valid_df, test_df, batch_size)\n",
        "\n",
        "except:\n",
        "    print('Invalid Input')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGNMs1NMwpym"
      },
      "source": [
        "#### **Display Image Sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T18:43:27.784411Z",
          "iopub.status.busy": "2023-03-03T18:43:27.784081Z",
          "iopub.status.idle": "2023-03-03T18:43:30.384676Z",
          "shell.execute_reply": "2023-03-03T18:43:30.38264Z",
          "shell.execute_reply.started": "2023-03-03T18:43:27.784384Z"
        },
        "id": "AGpOqNp-wpym",
        "outputId": "155e0bd8-332f-4d17-9e76-a9caf86d9b38"
      },
      "outputs": [
        {
          "data": {
            "image/png": "ibXBenM02fT4HmRqMRa0mQLJ/PZ8odMBBQ1ECW5gbhwcFBlEBJWXHr8WkMzqG0qH8IyOPKD9DalYqz2wnece5T1j5gmKfcU2fXDRrSKjudThj55XJZn3",
            "text/plain": [
              "<Figure size 2000x2000 with 25 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_images(train_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wvOKjeRGG65"
      },
      "source": [
        "#### **Generic Model Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T18:43:30.385968Z",
          "iopub.status.busy": "2023-03-03T18:43:30.385668Z",
          "iopub.status.idle": "2023-03-03T18:43:39.110712Z",
          "shell.execute_reply": "2023-03-03T18:43:39.109573Z",
          "shell.execute_reply.started": "2023-03-03T18:43:30.385937Z"
        },
        "id": "kDT4CV15abWT",
        "outputId": "47f58f86-652f-46d5-df2d-a65c4a3352d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "43941136/43941136 [==============================] - 0s 0us/step\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnetb3 (Functional  (None, 1536)              10783535  \n",
            " )                                                               \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 1536)              6144      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               393472    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11184179 (42.66 MB)\n",
            "Trainable params: 11093804 (42.32 MB)\n",
            "Non-trainable params: 90375 (353.03 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create Model Structure\n",
        "img_size = (224, 224)\n",
        "channels = 3\n",
        "img_shape = (img_size[0], img_size[1], channels)\n",
        "class_count = len(list(train_gen.class_indices.keys())) # to define number of classes in dense layer\n",
        "\n",
        "# create pre-trained model (you can built on pretrained model such as :  efficientnet, VGG , Resnet )\n",
        "# we will use efficientnetb3 from EfficientNet family.\n",
        "base_model = tf.keras.applications.efficientnet.EfficientNetB3(include_top= False, weights= \"imagenet\", input_shape= img_shape, pooling= 'max')\n",
        "\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001),\n",
        "    Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),\n",
        "                bias_regularizer= regularizers.l1(0.006), activation= 'relu'),\n",
        "    Dropout(rate= 0.45, seed= 123),\n",
        "    Dense(class_count, activation= 'softmax')\n",
        "])\n",
        "\n",
        "model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TciwhdM1GG66"
      },
      "source": [
        "#### **Set Callback Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-03T18:43:39.114291Z",
          "iopub.status.busy": "2023-03-03T18:43:39.11397Z",
          "iopub.status.idle": "2023-03-03T18:43:39.529742Z",
          "shell.execute_reply": "2023-03-03T18:43:39.528556Z",
          "shell.execute_reply.started": "2023-03-03T18:43:39.114263Z"
        },
        "id": "7abvdv7mGG66"
      },
      "outputs": [],
      "source": [
        "batch_size = 40   # set batch size for training\n",
        "epochs = 40   # number of all epochs in training\n",
        "patience = 1   #number of epochs to wait to adjust lr if monitored value does not improve\n",
        "stop_patience = 3   # number of epochs to wait before stopping training if monitored value does not improve\n",
        "threshold = 0.9   # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
        "factor = 0.5   # factor to reduce lr by\n",
        "ask_epoch = 5   # number of epochs to run before asking if you want to halt training\n",
        "batches = int(np.ceil(len(train_gen.labels) / batch_size))    # number of training batch to run per epoch\n",
        "\n",
        "callbacks = [MyCallback(model= model, patience= patience, stop_patience= stop_patience, threshold= threshold,\n",
        "            factor= factor, batches= batches, epochs= epochs, ask_epoch= ask_epoch )]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ap89fjdxGG67"
      },
      "source": [
        "#### **Train model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T18:43:39.531725Z",
          "iopub.status.busy": "2023-03-03T18:43:39.53135Z",
          "iopub.status.idle": "2023-03-03T19:14:14.969198Z",
          "shell.execute_reply": "2023-03-03T19:14:14.967992Z",
          "shell.execute_reply.started": "2023-03-03T18:43:39.531684Z"
        },
        "id": "Dy9tvPc1wpyn",
        "outputId": "abe82865-70ed-47a0-84f8-9c0642f84339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do you want model asks you to halt the training [y/n] ?\n",
            " Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n",
            " 1 /40     8.020   50.781   7.39985  49.414   0.00100  0.00100  accuracy     0.00    129.31 \n",
            "training elapsed time was 0.0 hours, 48.0 minutes, 42.44 seconds)\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(x= train_gen, epochs= epochs, verbose= 0, callbacks= callbacks,\n",
        "                    validation_data= valid_gen, validation_steps= None, shuffle= False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNKq6ebOGG67"
      },
      "source": [
        "#### **Display model performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T19:14:14.973753Z",
          "iopub.status.busy": "2023-03-03T19:14:14.972774Z",
          "iopub.status.idle": "2023-03-03T19:14:15.49821Z",
          "shell.execute_reply": "2023-03-03T19:14:15.496994Z",
          "shell.execute_reply.started": "2023-03-03T19:14:14.973712Z"
        },
        "id": "L0Bj0Sp_GG68",
        "outputId": "f7fd75b3-3b78-45c6-d60c-24a55ee262c2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 2000x800 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_training(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MySXhfAJGG68"
      },
      "source": [
        "# **Evaluate model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T19:14:15.500416Z",
          "iopub.status.busy": "2023-03-03T19:14:15.499998Z",
          "iopub.status.idle": "2023-03-03T19:14:28.494376Z",
          "shell.execute_reply": "2023-03-03T19:14:28.49328Z",
          "shell.execute_reply.started": "2023-03-03T19:14:15.500376Z"
        },
        "id": "wSKDkyXXGG68",
        "outputId": "ceac3c18-abdd-4979-f420-66fe460d884a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9/9 [==============================] - 2s 158ms/step - loss: 0.0988 - accuracy: 1.0000\n",
            "9/9 [==============================] - 2s 159ms/step - loss: 0.1109 - accuracy: 0.9917\n",
            "9/9 [==============================] - 5s 193ms/step - loss: 0.1251 - accuracy: 0.9922\n",
            "Train Loss:  0.09884872287511826\n",
            "Train Accuracy:  1.0\n",
            "--------------------\n",
            "Validation Loss:  0.1108979880809784\n",
            "Validation Accuracy:  0.9916666746139526\n",
            "--------------------\n",
            "Test Loss:  0.1251276731491089\n",
            "Test Accuracy:  0.9922027587890625\n"
          ]
        }
      ],
      "source": [
        "ts_length = len(test_df)\n",
        "test_batch_size = test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))\n",
        "test_steps = ts_length // test_batch_size\n",
        "\n",
        "train_score = model.evaluate(train_gen, steps= test_steps, verbose= 1)\n",
        "valid_score = model.evaluate(valid_gen, steps= test_steps, verbose= 1)\n",
        "test_score = model.evaluate(test_gen, steps= test_steps, verbose= 1)\n",
        "\n",
        "print(\"Train Loss: \", train_score[0])\n",
        "print(\"Train Accuracy: \", train_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Validation Loss: \", valid_score[0])\n",
        "print(\"Validation Accuracy: \", valid_score[1])\n",
        "print('-' * 20)\n",
        "print(\"Test Loss: \", test_score[0])\n",
        "print(\"Test Accuracy: \", test_score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4l-DABtFGG68"
      },
      "source": [
        "# **Get Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T19:14:28.496253Z",
          "iopub.status.busy": "2023-03-03T19:14:28.495791Z",
          "iopub.status.idle": "2023-03-03T19:14:32.811345Z",
          "shell.execute_reply": "2023-03-03T19:14:32.810289Z",
          "shell.execute_reply.started": "2023-03-03T19:14:28.496216Z"
        },
        "id": "GDFj7MZdGG69",
        "outputId": "f6a6713c-da92-41a3-cbd1-8b665aca9c0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 3 3 3 3 3 3 2 3 2 2 2 2 2 3 2 2 2 2 3 3 3 2 2 3 2 0 2 3 3 2 2 3 3 2 0 2\n",
            " 3 2 2 0 0 2 3 3 2 3 3 3 2 3 1 2 2 3 2 2 3 3 2 3 2 3 0 3 2 2 3 3 2 2 3 0 3\n",
            " 3 2 3 3 2 2 0 2 3 2 3 2 3 2 3 1 3 3 2 3 2 2 2 2 3 2 2 2 0 0 2 2]\n"
          ]
        }
      ],
      "source": [
        "preds = model.predict_generator(test_gen)\n",
        "y_pred = np.argmax(preds, axis=1)\n",
        "print(y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJscUTF6GG69"
      },
      "source": [
        "#### **Confusion Matrics and Classification Report**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2023-03-03T19:14:32.814948Z",
          "iopub.status.busy": "2023-03-03T19:14:32.813961Z",
          "iopub.status.idle": "2023-03-03T19:14:33.22508Z",
          "shell.execute_reply": "2023-03-03T19:14:33.224062Z",
          "shell.execute_reply.started": "2023-03-03T19:14:32.814904Z"
        },
        "id": "tQR-UlD6GG69",
        "outputId": "445e6811-a51f-40df-f29b-8ebcb48b941c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix, Without Normalization\n",
            "[[ 71   0   1   0]\n",
            " [  0   5   0   0]\n",
            " [  0   0 255   1]\n",
            " [  0   0   2 178]]\n",
            "                  precision    recall  f1-score   support\n",
            "\n",
            "    MildDemented       1.00      0.99      0.99        72\n",
            "ModerateDemented       1.00      1.00      1.00         5\n",
            "     NonDemented       0.99      1.00      0.99       256\n",
            "VeryMildDemented       0.99      0.99      0.99       180\n",
            "\n",
            "        accuracy                           0.99       513\n",
            "       macro avg       1.00      0.99      0.99       513\n",
            "    weighted avg       0.99      0.99      0.99       513\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9EAAAOpCAYAAAAdsAAAAASUVORK5CYI",
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "g_dict = test_gen.class_indices\n",
        "classes = list(g_dict.keys())\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(test_gen.classes, y_pred)\n",
        "plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix')\n",
        "\n",
        "# Classification report\n",
        "print(classification_report(test_gen.classes, y_pred, target_names= classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qDm2K3wnwpyo"
      },
      "outputs": [],
      "source": [
        "model.save(\"model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO2YROK6wpyo"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 2029496,
          "sourceId": 3364939,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30407,
      "isGpuEnabled": true,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
